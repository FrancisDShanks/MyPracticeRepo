{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.ensemble import BaggingClassifier,BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor\n",
    "from sklearn.ensemble import voting_classifier,VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split,learning_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "test = pd.read_csv(\"./data/test.csv\")\n",
    "# set sort=False because test data don't have survived data\n",
    "combine = pd.concat([train,test],sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name length\n",
    "combine['Name_len'] = combine['Name'].apply(lambda x:len(x))\n",
    "combine['Name_len'] = pd.qcut(combine['Name_len'],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine['Title'] = combine['Name'].apply(lambda x: x.split(', ')[1]).apply(lambda x: x.split('.')[0])\n",
    "combine.Title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title from Name\n",
    "combine['Title'] = combine['Name'].apply(lambda x: x.split(', ')[1]).apply(lambda x: x.split('.')[0])\n",
    "combine['Title'] = combine['Title'].replace(['Master','Major','Col','Sir','Dr', 'Col'],'Royalty')\n",
    "combine['Title'] = combine['Title'].replace(['Rev','Don','Capt','Jonkheer'],'Goodman')\n",
    "combine['Title'] = combine['Title'].replace(['Mlle','Ms','Dona'], 'Miss')\n",
    "combine['Title'] = combine['Title'].replace(['the Countess','Mme','Lady'], 'Mrs')\n",
    "df = pd.get_dummies(combine['Title'],prefix='Title')\n",
    "combine = pd.concat([combine,df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family and family name\n",
    "combine['Fname'] = combine['Name'].apply(lambda x:x.split(',')[0])\n",
    "combine['Familysize'] = combine['SibSp'] + combine['Parch']\n",
    "dead_female_Fname = list(set(combine[(combine.Sex=='female') & (combine.Age>=12)\n",
    "                              & (combine.Survived==0) & (combine.Familysize>1)]['Fname'].values))\n",
    "survive_male_Fname = list(set(combine[(combine.Sex=='male') & (combine.Age>=12)\n",
    "                              & (combine.Survived==1) & (combine.Familysize>1)]['Fname'].values))\n",
    "combine['Dead_female_family'] = np.where(combine['Fname'].isin(dead_female_Fname),1,0)\n",
    "combine['Survive_male_family'] = np.where(combine['Fname'].isin(survive_male_Fname),1,0)\n",
    "combine = combine.drop(['Name','Fname'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "# discretize age, add new feature 'IsChild'\n",
    "group = combine.groupby(['Title', 'Pclass'])['Age']\n",
    "combine['Age'] = group.transform(lambda x: x.fillna(x.median()))\n",
    "combine = combine.drop('Title',axis=1)\n",
    "combine['IsChild'] = np.where(combine['Age']<=12,1,0)\n",
    "combine['Age'] = pd.cut(combine['Age'],5)\n",
    "# combine = combine.drop('Age',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding 'Familysize'\n",
    "combine['Familysize'] = np.where(combine['Familysize']==0, 'solo',\n",
    "                                    np.where(combine['Familysize']<=3, 'normal', 'big'))\n",
    "df = pd.get_dummies(combine['Familysize'],prefix='Familysize')\n",
    "combine = pd.concat([combine,df],axis=1).drop(['SibSp','Parch','Familysize'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cabin\n",
    "combine['Cabin_isNull'] = np.where(combine['Cabin'].isnull(),0,1)\n",
    "combine = combine.drop('Cabin',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked, fill NaN with S\n",
    "# because there is only two NaN, it will not help a lot to set NaN as a category \n",
    "combine.Embarked = combine.Embarked.fillna('C')\n",
    "df = pd.get_dummies(combine['Embarked'],prefix='Embarked')\n",
    "combine = pd.concat([combine,df],axis=1).drop('Embarked',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pclass, one hot encoding\n",
    "df = pd.get_dummies(combine['Pclass'],prefix='Pclass')\n",
    "combine = pd.concat([combine,df],axis=1).drop('Pclass',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sex\n",
    "df = pd.get_dummies(combine['Sex'],prefix='Sex')\n",
    "combine = pd.concat([combine,df],axis=1).drop('Sex',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare\n",
    "combine['Fare'] = pd.qcut(combine.Fare,3)\n",
    "df = pd.get_dummies(combine.Fare,prefix='Fare')\n",
    "combine = pd.concat([combine,df],axis=1).drop('Fare',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticket\n",
    "combine['Ticket_Lett'] = combine['Ticket'].apply(lambda x: str(x)[0])\n",
    "combine['Ticket_Lett'] = combine['Ticket_Lett'].apply(lambda x: str(x))\n",
    "\n",
    "combine['High_Survival_Ticket'] = np.where(combine['Ticket_Lett'].isin(['1', '2', 'P']),1,0)\n",
    "combine['Low_Survival_Ticket'] = np.where(combine['Ticket_Lett'].isin(['A','W','3','7']),1,0)\n",
    "combine = combine.drop(['Ticket','Ticket_Lett'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = combine.drop([\"PassengerId\",\"Survived\"], axis=1).columns\n",
    "le = preprocessing.LabelEncoder()\n",
    "for feature in features:\n",
    "    le = le.fit(combine[feature])\n",
    "    combine[feature] = le.transform(combine[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine.rename(columns={\n",
    "    \"Fare_(-0.001, 8.662]\":\"Fare_0\",\n",
    "    \"Fare_(8.662, 26.0]\":\"Fare_1\",\n",
    "    \"Fare_(26.0, 512.329]\":\"Fare_2\"\n",
    "},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = combine.iloc[:891,:].drop([\"PassengerId\",\"Survived\"], axis=1)\n",
    "Y_all = combine.iloc[:891,:][\"Survived\"]\n",
    "X_test = combine.iloc[891:,:].drop([\"PassengerId\",\"Survived\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "svc = SVC()\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "dt = DecisionTreeClassifier()\n",
    "rf0 = RandomForestClassifier(n_estimators=300,min_samples_leaf=4,class_weight={0:0.745,1:0.255})\n",
    "rf = RandomForestClassifier(n_estimators = 750, criterion = 'gini', max_features = 'sqrt',\n",
    "                                             max_depth = 3, min_samples_split = 4, min_samples_leaf = 2,\n",
    "                                             n_jobs = 50, random_state = 42, verbose = 1)\n",
    "gbdt = GradientBoostingClassifier(n_estimators=500,learning_rate=0.03,max_depth=3)\n",
    "gbm_est = GradientBoostingClassifier(n_estimators=900, learning_rate=0.0008, loss='exponential',\n",
    "                                                  min_samples_split=3, min_samples_leaf=2, max_features='sqrt',\n",
    "                                                  max_depth=3, random_state=42, verbose=1)\n",
    "xgbGBDT = XGBClassifier(max_depth=5, n_estimators=300, learning_rate=0.05)\n",
    "clfs = [lr, svc, knn, dt, rf0, rf, gbdt, gbm_est, xgbGBDT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = XGBClassifier(learning_rate=0.1,max_depth=2,silent=True,objective='binary:logistic')\n",
    "param_test = {\n",
    "    'n_estimators':[50,100,300,700],\n",
    "    'max_depth':[1,2,3,5]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=clf,param_grid=param_test,scoring='accuracy',cv=50)\n",
    "grid_search.fit(X_all,Y_all)\n",
    "grid_search.grid_scores_, grid_search.best_params_, grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = VotingClassifier(estimators = [('lr',lr),('svc', svc),('rf', rf),('gbm_est',gbm_est),('xgbGBDT',xgbGBDT)],\n",
    "                                       voting = 'hard',n_jobs = -1)\n",
    "voting.fit(X_all,Y_all)\n",
    "predictions = voting.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test[\"PassengerId\"],\n",
    "    'Survived': predictions.astype(np.int32)\n",
    "})\n",
    "submission.to_csv('./v4.csv',mode='w+',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
